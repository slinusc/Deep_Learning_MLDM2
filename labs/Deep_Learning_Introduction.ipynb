{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# ML import\n",
    "from sklearn.model_selection import train_test_split # Splitting the data set\n",
    "from sklearn.preprocessing import MinMaxScaler       # Normalization\n",
    "from sklearn.preprocessing import LabelEncoder       # Encoder\n",
    "import torch                                   # PyTorch\n",
    "import torch.nn as nn                          # PyTorch building blocks\n",
    "from IPython.display import YouTubeVideo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f82135620898b56",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Nets Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6ac651518be031a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Application: Predicting house prices\n",
    "\n",
    "In this application, we will implement a Shallow Neural Network model with PyTorch to predict house prices using the [Ames Housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0cbbb48424f4ea2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eba2e35f5274d3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import data\n",
    "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/JasminaZHAW/MLDM2/main/labs/data/house_price.csv\")\n",
    "raw_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c35558741b0e0c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "raw_data.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a86ef4804462a79",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains 81 columns. A description of the features is available in the file \"[house_price_data_description](https://github.com/michalis0/DataScience_and_MachineLearning/blob/master/Week_5/data/house_price_data_description.txt)\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "220e49c8d14b345e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76bb34e8b3694e60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first extract the features of interest. We will use the numeric columns:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6bc54fae5f22513"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data types\n",
    "raw_data.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d063f32cf1f43146",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display numeric features (integer and floats)\n",
    "numeric_columns = list(raw_data.columns[(raw_data.dtypes==np.int64) |\n",
    "                 (raw_data.dtypes==np.float64)])\n",
    "print(numeric_columns, \"\\n\", len(numeric_columns))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa518b9d231c259",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SalePrice` is the value we want to predict. We set it as the last column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3578835ba236eb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Output SalePrice as last column\n",
    "numeric_columns.remove('SalePrice')\n",
    "numeric_columns.append('SalePrice')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b46bc3596dc53c6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also remove the `Id` column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa3b6943f531d8c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Remove Id\n",
    "numeric_columns.remove('Id')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da3ae6d4afa93b83",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we extract the numeric data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9655c85b5996ab52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract numeric data\n",
    "numeric_data = raw_data[numeric_columns]\n",
    "numeric_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6459fcb0fa0e3c5c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's deal with the missing values in the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4128b5f54a886a06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display features with missing values\n",
    "nan_columns = np.any(pd.isna(numeric_data), axis = 0)\n",
    "nan_columns = list(nan_columns[nan_columns == True].index)\n",
    "nan_columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7086f53365cfc19b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We simply replace them with zero."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3462eb810f69d5ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Replace NAN with 0\n",
    "numeric_data = numeric_data.fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9774b98b0b6c08a8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Creating training and test set\n",
    "\n",
    "Let's split the data for training and test. We use the `train_test_split` module of `sklearn`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2e81368f4a3317"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Splitting training/test set\n",
    "numeric_data_train, numeric_data_test = train_test_split(numeric_data, test_size=0.1, random_state=7)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffa114ed471a412",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Normalizing the data\n",
    "\n",
    "Before training our model, we need to normalize the data. We do this by subtracting each column from its minimum value and then dividing it by the difference between maximum and minimum. We use the `MinMaxScaler` of `sklearn`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64b263bb796a28f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "#Fit the scaler\n",
    "scaler.fit(numeric_data_train)\n",
    "#Transform the train and the test set\n",
    "numeric_data_train.loc[:,:] = scaler.transform(numeric_data_train)\n",
    "numeric_data_test.loc[:,:] = scaler.transform(numeric_data_test)\n",
    "\n",
    "numeric_data_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64682141c6aecd44",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we split the column we want to predict (\"SalePrice\") to our features:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9667834f31cf75"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract features and output\n",
    "numeric_x_columns = list(numeric_data_train.columns)\n",
    "numeric_x_columns.remove(\"SalePrice\")\n",
    "X_train_df = numeric_data_train[numeric_x_columns]\n",
    "y_train_df = pd.DataFrame(numeric_data_train[\"SalePrice\"])\n",
    "X_test_df = numeric_data_test[numeric_x_columns]\n",
    "y_test_df = pd.DataFrame(numeric_data_test[\"SalePrice\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50cdbd0e3a7182f0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, all set, we can start building our Neural Net!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aa575968143d657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Building a Shallow Neural Network model with PyTorch\n",
    "\n",
    "We use the `PyTorch` library ([Documentation](https://pytorch.org/), imported at the beginning of this notebook with the following lines of codes:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "```\n",
    "\n",
    "`torch.nn` contains the building blocks to build Neural Nets, e.g., the layers ([Documentation](https://pytorch.org/docs/stable/nn.html))."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d3b01686385b72d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Create tensors\n",
    "\n",
    "The first step is to convert the data into torch tensors. A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type. It's very similar to arrays in `NumPy`.\n",
    "\n",
    "We rely on `torch.tensor()` for the conversion ([Documentation](https://pytorch.org/docs/stable/tensors.html))."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0254d04b829d3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train_df.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test_df.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0649a635d70393",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(X_train.size(), y_train.size())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b253d181acb2699",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Define and train a model with PyTorch\n",
    "\n",
    "A model is defined as a `class` in PyTorch. Classes are a means of bundling data and functionality together, allowing to create a new type of Python object. You can read the Python documentation on [Classes](https://docs.python.org/3/tutorial/classes.html) to learn more about them.\n",
    "\n",
    "When you create your Neural Net, you should define:\n",
    "- a `__init__` function in which you define the layers of your network.\n",
    "- a `forward` function (method) that defines the forward pass on the network.\n",
    "\n",
    "Now the goal is to create a single layer network with ReLU activation:\n",
    "- The layer `nn.Linear()` performs a linear transformation ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). The input and output are the number of neurons\n",
    "- `nn.ReLU()` applies the Rectified Linear Unit function: $ReLU(x)=\\max(0,x)$ ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html))."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6522214f376c2208"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer\n",
    "        self.linear2 = nn.Linear(H1, D_out)       # Linear transformation for output layer\n",
    "        self.activation = nn.ReLU()               # Activation function for hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))   # Hidden layer: linear transformation + ReLU\n",
    "        y_pred = self.linear2(y_pred)               # Output layer: linear transformation\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f79aa8297218b93",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`D_in` is the input dimension, i.e., the number of features. Similarly, `D_out` is the output dimension, i.e., 1 (we only predict the \"SalePrice\"):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357809ccf3f7b601"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "D_in, D_out = X_train.shape[1], y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39d2e7d41128ef5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, let's define our first model. It is an instance of our newly-created class \"Net\". We are going to use 500 neurons for the hidden layer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de708d308a86fb77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Model with 500 neurons\n",
    "model1 = Net(D_in, 500, D_out)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e708699750e9da3a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's calculate now how many parameters we have in the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "627b3ead7eb703fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate how many parameters are in the model\n",
    "pytorch_total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af21b54b5519092",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next steps is to define the **loss criterion** and the **optimizer** for the network. That is, we have to define the loss function we want to optimize during training and also the optimization method. We use:\n",
    "- `MSELoss()` as loss criterion, i.e., the mean square error ([Documentation](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))\n",
    "- `SGD()`as optimizer, i.e., stochastic gradient descent ([Documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78294354a52343c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4e748706a17b7d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wonderful, we are ready to do the training! We can simply by looping over the number of iterations. The training has 3 main steps:\n",
    "- A forward pass to compute the prediction for the current data point (batch).\n",
    "- Computing the loss for the current prediction with the previously defined criterion.\n",
    "- A backward pass to compute the gradient of the loss with respect to the weight of the network (`backward()`)\n",
    "- Finally, updating the weights of the network (`optimizer.step()`).\n",
    "\n",
    "Note that in each backward pass PyTorch saves the gradient for all of the parameters. Therefore it is important to replace the old gradient values with zero in the beginning of each iteration (`optimizer.zero_grad()`), otherwise the gradients will be accumulated during the iterations!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfe49d6c12b865e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses1 = []\n",
    "losses1_test = []\n",
    "\n",
    "for t in range(500):                # 500 iterations\n",
    "\n",
    "    # Forward pass: compute prediction on training set\n",
    "    y_pred = model1(X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "\n",
    "    # Compute gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute loss on test set\n",
    "    losses1_test.append(criterion(model1(X_test), y_test).item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3e4f110f80d2f2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the evolution of the MSE on the training set and test set:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f0598c8fafa27c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(losses1, label=\"Training loss\")\n",
    "plt.plot(losses1_test, label=\"Test loss\")\n",
    "plt.title('Evolution of training and test loss - 500 neurons')\n",
    "plt.ylim(top=70, bottom=0.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5619faf6ceba7c46",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try a new model with more neurons in the hidden layer. We use 1000 neurons, and follow the same steps as before:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1771e9215828bc0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model2 = Net(D_in, 1000, D_out)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34eda8ae9dcda491",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a5bd75d07d31cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses2 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model2(X_train)\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses2.append(loss.item())\n",
    "\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d516f3bcd9f04411",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's visualize the evolution of the training loss for the two models:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0da57191a92ee7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(losses1, label=\"Model 1: 500 neurons\")\n",
    "plt.plot(losses2, label=\"Model 2: 1000 neurons\")\n",
    "plt.title('Evolution of training loss')\n",
    "plt.ylim(top=70, bottom=0.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e60b7efb311014c8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare the MSE loss on the test data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82b0820c039e83f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# prediction for model 1\n",
    "model1_pred = model1(X_test)\n",
    "print(\"MSE loss for model 1: \", criterion(model1_pred, y_test))\n",
    "# prediction for model 2\n",
    "model2_pred = model2(X_test)\n",
    "print(\"MSE loss for model 2: \", criterion(model2_pred, y_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc66311c7ac719dd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment: Predict the streams of a song with a 2 layer neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a68442daee064ede"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on this dataset ([link](https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Week_5/data/spotify.csv)) imported from kaggle, create a Neural Network that will predict the streams of a song based on its bpm, key, danceability, valence, energy, acousticness, instrumentalness, liveness, and speechiness. (Hint: if there are some errors, try to copy/paste them in Google to see some solutions.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d934572cdf8769db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#import the dataset\n",
    "df_lab = pd.read_csv(\"https://raw.githubusercontent.com/JasminaZHAW/MLDM2/main/labs/data/spotify.csv\", encoding='latin-1', index_col=\"Unnamed: 0\")\n",
    "display(df_lab)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "156a2be971524213",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_lab.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9311e90a5690fb4a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Entfernen nicht ben√∂tigter Spalten\n",
    "rem_col = [\"bpm\", \"key\", \"mode\", \"artist_count\", \"released_year\", \"released_month\", \"released_day\"]\n",
    "df_lab.drop(rem_col, axis=1, inplace=True)\n",
    "\n",
    "# Erstellen einer neuen Indexspalte, die track_name und artist(s)_name kombiniert\n",
    "df_lab['index'] = df_lab['track_name'].astype(str) + ' - ' + df_lab['artist(s)_name'].astype(str)\n",
    "\n",
    "# Setzen der neuen Indexspalte als Index\n",
    "df_lab.set_index('index', inplace=True)\n",
    "\n",
    "# Entfernen der nun redundanten Spalten 'track_name' und 'artist(s)_name'\n",
    "df_lab.drop(['track_name', 'artist(s)_name'], axis=1, inplace=True)\n",
    "\n",
    "# Verschieben der 'streams'-Spalte an die erste Position\n",
    "columns = ['streams'] + [col for col in df_lab.columns if col != 'streams']\n",
    "df_lab = df_lab[columns]\n",
    "\n",
    "# Umwandeln der Datentypen in float\n",
    "df_lab = df_lab.astype(float)\n",
    "\n",
    "# Anzeigen des verarbeiteten DataFrames\n",
    "df_lab"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dab29e47cef587ea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking the correlation between the streams and the other features,by using the Spearman correlation coefficient for monotonic relationships:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44a00ddd02420161"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# checking correlation between streams and the other features\n",
    "correlation = df_lab.corr(method=\"spearman\")\n",
    "correlation['streams'].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f077cb56dbe9a967",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# scaling data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_lab)\n",
    "df = scaler.transform(df_lab)\n",
    "# split the training/test dataset\n",
    "train, test = train_test_split(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58799c3ad40214a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# extract features and output\n",
    "train_y = train[:,0]\n",
    "train_x = train[:,1:]\n",
    "test_y = test[:,0]\n",
    "test_x = test[:,1:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eca65fcc4fc59ba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reshape the output to be a column vector\n",
    "train_y = train_y.reshape(-1, 1)\n",
    "test_y = test_y.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5926f179177ae7d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# testing a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create a model\n",
    "model = LinearRegression()\n",
    "# fit the model\n",
    "model.fit(train_x, train_y)\n",
    "# make predictions\n",
    "y_pred = model.predict(test_x)\n",
    "# calculate the mean squared error\n",
    "print(\"MSE loss for linear regression: \", mean_squared_error(test_y, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b4e287c145a45e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now creating a Neural Net class with two hidden layer using the ReLU activation in both of them:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8225b37c38e02cc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# first transform the data into tensor\n",
    "X_train = torch.tensor(train_x, dtype=torch.float)\n",
    "y_train = torch.tensor(train_y, dtype=torch.float)\n",
    "X_test = torch.tensor(test_x, dtype=torch.float)\n",
    "y_test = torch.tensor(test_y, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46904ad165c14e5e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)        # Linear transformation for hidden layer 1\n",
    "        self.linear2 = nn.Linear(H1, H2)          # Linear transformation for hidden layer 2\n",
    "        self.linear3 = nn.Linear(H2, D_out)       # Linear transformation for output layer\n",
    "        self.activation = nn.ReLU()               # Activation function for hidden layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))  # Hidden layer 1: linear transformation + ReLU\n",
    "        y_pred = self.activation(self.linear2(y_pred))  # Hidden layer 2: linear transformation + ReLU\n",
    "        y_pred = self.linear3(y_pred)                   # Output layer: linear transformation\n",
    "        return y_pred\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f76a4c6bafa18f4d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create a model and give the right dimension to the input and output layers\n",
    "model1 = Net(len(X_train[0]),100, 200, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73199a5b6dd6c06e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate how many parameters the model has\n",
    "pytorch_total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9bd37264a6754cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# MSELoss() as the loss criterion and Adam() as the optimizer\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001) # Learing rate 0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b853cdf41fea19d7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now create a loop to train your model (with 500 iterations), don't forget to save your loss criterion in order to plot it later !"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "454ba1d189fff34d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "losses1 = []\n",
    "losses1_test = []\n",
    "\n",
    "for t in range(200):                # 100 iterations\n",
    "\n",
    "    # Forward pass: compute prediction on training set\n",
    "    y_pred = model1(X_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    #print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "\n",
    "    # Compute gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute loss on test set\n",
    "    losses1_test.append(criterion(model1(X_test), y_test).item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33daca4cf11b4ae7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(losses1, label=\"Training loss\")\n",
    "plt.plot(losses1_test, label=\"Test loss\")\n",
    "plt.title('Evolution of training and test loss')\n",
    "#plt.ylim(top=70, bottom=0.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "-OYcTnB2Q8ke",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "outputId": "51dc1015-ca09-4180-de88-7c0c2850bab3"
   },
   "id": "-OYcTnB2Q8ke",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# MSE loss for the model\n",
    "print(\"MSE loss for model 1: \", losses1_test[-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66cb64e584737bc8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
